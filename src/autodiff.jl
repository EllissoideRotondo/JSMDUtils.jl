
module Autodiff 

import ForwardDiff: Dual, ForwardDiff, 
                    GradientConfig, JacobianConfig, HessianConfig

"""
    JSMDDiffTag 

Singleton to be used as a ForwardDiff tag for the JSMD ecosystem.
"""
struct JSMDDiffTag end 

"""
    gentag(x)

Generate a dual tag for the element type of `x`.
"""
@inline gentag(x) = ForwardDiff.Tag(JSMDDiffTag(), eltype(x))

# -------------------------
# Derivatives Operations 

"""
    derivative(f, x::Number)

Return `df/dx` evaluated at `x`, assuming `f` is called as `f(x)`. 

!!! note 
    This function assumes that `isa(f(x), Union{Real, AbstractArray})`.
    
!!! warning 
    The dual numbers generated by this function are tagged with the 
    [`Autodiff.JSMDDiffTag`](@ref) which could lead to perturbation confusion in certain 
    scenarios. Hence, this derivative operation should be used with care.

### See also 
See also [`Autodiff.gradient!`](@ref) and [`Autodiff.hessian!`](@ref)
"""
function derivative(f, x::Number)
    
    # Create the Tag type for the dual number 
    T = typeof(gentag(x))

    # Create the dual number 
    xdual = Dual{T}(x, one(x))

    # Compute the derivative
    return ForwardDiff.extract_derivative(T, f(xdual))

end


# -------------------------
# Gradient Operations

"""
    AutoGradientConfig(x)

Generate a [`ForwardDiff.GradientConfig`](@ref) instance based on the type and shape of the 
input `x`. 

The returned `GradientConfig` instance contains all the work buffers required by 
[`Autodiff.gradient!`](@ref). The chunk size is automatically set to 1 since it is part of 
the type. Having a larger chunk-size improves performance but would require a greater 
number of dispatches for the function wrappers. 

!!! warning 
    The dual numbers that are generated from this instance all share the 
    ['Autodiff.JSMDDiffTag`](@ref) tag, which could lead to perturbation confusion if the 
    gradient operation is not used with care.

### See also 
See also [`Autodiff.gradient!`](@ref)

"""
@inline function AutoGradientConfig(x) 
    GradientConfig(nothing, x, ForwardDiff.Chunk{1}(), gentag(x))
end

""" 
    gradient!(res, f, x)
    gradient!(res, f, x, cfg::GradientConfig)

Compute `∇f` evaluated at `x'` and store the output in `res`, assuming `f` is called as 
`f(x)`. To avoid runtime allocations, a `GradientConfig` instance must be provided. 

!!! note 
    This function assumes that `isa(f(x), Real)`.

!!! warning 
    If the gradient config has been setup with [`Autodiff.AutoGradientConfig`](@ref), 
    the dual numbers generated by this function are tagged with the [`Autodiff.JSMDDiffTag`](@ref) 
    which could lead to perturbation confusion in certain scenarios. Hence, this gradient 
    operation should be used with care.

### See Also 
See also [`Autodiff.AutoGradientConfig`](@ref)

"""
@inline gradient!(res, f, x) = gradient!(res, f, x, AutoGradientConfig(x))

function gradient!(res, f, x::AbstractArray{<:Number}, cfg::GradientConfig)
    ForwardDiff.chunk_mode_gradient!(res, f, x, cfg)
    nothing
end


# -------------------------
# Jacobian Operations

"""
    AutoJacobianConfig(x)

Generate a [`ForwardDiff.JacobianConfig`](@ref) instance based on the type and shape of the 
input vector `x`. 

The returned `JacobianConfig` instance contains all the work buffers required by 
[`Autodiff.jacobian!`](@ref) when the target function takes the form `f(x)`. The chunk 
size is automatically set to 1 since it is part of the type. Having a larger chunk-size 
improves performance but would require a greater number of dispatches for the function 
wrappers. 
    
---

    AutoJacobianConfig(y, x)

Generate a [`ForwardDiff.JacobianConfig`](@ref) instance based on the type and shape of the 
input vector `x` and the output vector `y`. 

The returned `JacobianConfig` instance contains all the work buffers required by 
[`Autodiff.jacobian!`](@ref) when the target function takes the in-place form `f!(y, x).` 
    
!!! warning 
    The dual numbers that are generated from this instance all share the 
    ['Autodiff.JSMDDiffTag`](@ref) tag, which could lead to perturbation confusion if the 
    gradient operation is not used with care.

### See Also 
See also [`Autodiff.jacobian!`](@ref).

"""
@inline function AutoJacobianConfig(x)
    JacobianConfig(nothing, x, ForwardDiff.Chunk{1}(), gentag(x))
end

@inline function AutoJacobianConfig(y, x)
    JacobianConfig(nothing, y, x, ForwardDiff.Chunk{1}(), gentag(x))
end 


"""
    jacobian!(res, f, x)
    jacobian!(res, f, x, cfg::JacobianConfig)

Compute `J(f)` evaluated at the input vector `x` and store the result in `res`, assuming 
`f` is called as `f(x)`. To avoid runtime allocations, a `JacobianConfig` instance must be 
provided. 

!!! note 
    This function assumes that `isa(f(x), Union{AbstractArray})`.

---

jacobian!(res, f!, y, x)
jacobian!(res, f!, y, x, cfg::JacobianConfig)

Compute `J(f)` evaluated at the input vector `x` and store the result in `res`, assuming 
`f` is called as `f!(y, x)`, where the result is stored in `y`. To avoid runtime allocations,
a `JacobianConfig` instance must be  provided. 

!!! warning 
    If the jacobian config has been setup with [`Autodiff.AutoJacobianConfig`](@ref), 
    the dual numbers generated by this function are tagged with the [`Autodiff.JSMDDiffTag`](@ref) 
    which could lead to perturbation confusion in certain scenarios. Hence, this jacobian 
    operation should be used with care.

### See Also 
See also [`Autodiff.AutoJacobianConfig`](@ref)
"""
@inline jacobian!(res, f, x) = jacobian!(res, f, x, AutoJacobianConfig(x)) 
@inline jacobian!(res, f!, y, x) = jacobian!(res, f!, y, x, AutoJacobianConfig(y, x))

function jacobian!(res, f, x::AbstractArray{<:Number}, cfg::JacobianConfig)
    ForwardDiff.chunk_mode_jacobian!(res, f, x, cfg)
    nothing
end

function jacobian!(res, f!, y::AbstractArray, x::AbstractArray, cfg::JacobianConfig)
    ForwardDiff.chunk_mode_jacobian!(res, f!, y, x, cfg)
    nothing
end


# -------------------------
# Hessian Operations

"""
    AutoHessianConfig

Wrapper object around a `ForwardDiff.HessianConfig` type. 
"""
struct AutoHessianConfig{HC, V}
    cfg::HC
    y::V
end

"""
    AutoHessianConfig(x)

Generate a [`Autodiff.AutoHessianConfig`](@ref) instance based on the type and shape of the 
input vector `x`. 

The returned `AutoHessianConfig` instance contains all the work buffers required by 
[`Autodiff.hessian!`](@ref) when the target function takes the form `f(x)`. The chunk 
size is automatically set to 1 since it is part of the type. Having a larger chunk-size 
improves performance but would require a greater number of dispatches for the function 
wrappers. 

!!! warning 
    The dual numbers that are generated from this instance all share the 
    ['Autodiff.JSMDDiffTag`](@ref) tag, which could lead to perturbation confusion if the 
    hessian operation is not used with care.

### See Also 
See also [`Autodiff.hessian!`](@ref).
"""
function AutoHessianConfig(x)

    # Create the Tag for the dual operations 
    tag = gentag(x)
    chunk = ForwardDiff.Chunk{1}()

    jacobian_config = JacobianConfig(nothing, x, x, chunk, tag)
    gradient_config = GradientConfig(nothing, jacobian_config.duals[2], chunk, tag)

    hc = HessianConfig(jacobian_config, gradient_config)
    AutoHessianConfig(hc, similar(x))

end

"""
    hessian!(res, f, x)
    hessian!(res, f, x, ahc::AutoHessianConfig)

Compute `H(f)` (i.e., `J(∇(f))`) evaluated at `x'` and store the output in `res`, assuming 
`f` is called as `f(x)`. To avoid runtime allocations, an [`Autodiff.AutoHessianConfig`](@ref) 
instance must be provided. 

!!! note 
    This function assumes that `isa(f(x), Real)`.

!!! warning 
    The dual numbers generated by this function are tagged with the 
    [`Autodiff.JSMDDiffTag`](@ref) which could lead to perturbation confusion in certain 
    scenarios. Hence, this hessian operation should be used with care.

### See Also 
See also [`Autodiff.AutoHessianConfig`](@ref). 
"""
@inline hessian!(res, f, x) = hessian!(res, f, x, AutoHessianConfig(x))

function hessian!(res, f, x::AbstractArray, ahc::AutoHessianConfig)
    ∇f! = (t, x) -> gradient!(t, f, x, ahc.cfg.gradient_config)
    jacobian!(res, ∇f!, ahc.y, x, ahc.cfg.jacobian_config)
    nothing
end

end
